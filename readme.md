
# Research Prompt Comparison Tool
This tool is designed to compare the effectiveness of a modified prompt against the original prompt used in a research paper. It evaluates the number of correct answers generated by each prompt when applied to a specific task, allowing researchers to quantitatively assess the impact of their modifications.


## Features
- Prompt Evaluation: Compares the original research paper prompt with a modified version.
- Correct Answer Counting: Counts the number of correct answers generated by each prompt.
- JSONL Support: Reads and processes results stored in JSONL format.

## How It Works
The tool operates by reading through JSONL files that contain the results of running both the original and modified prompts. Each entry in these files includes whether the response was correct. The tool counts and compares the number of correct responses for each prompt.

## Usage
1. Ensure you have Python installed on your system.
2. Place your JSONL files in the designated folders. By default, these are named modified_prompt_results.jsonl and paper_prompt_results.jsonl.
3. Run the script using Python:

```
python vision_test.py
```



## Output
The script prints the count of correct answers for both the modified and original prompts, allowing for a direct comparison.

## Requirements
- Python 3.x
- JSONL files with the results of the prompt evaluations